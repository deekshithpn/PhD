
# Individual Neural networks
We use individual neural networks for mean, var, skw, kurt.
Note: this is similar to the 

## Calculation of loss
The loss fucntion here is caclculated for mean seperately, for varimce seperately, for skewness seperately and for kurtosis seperately.
We first calclate the dm/dt, dv/dt, ds/dt, dk/dt from the daa and generate the corresponding interpolation functions.
Suppose I am training the nn for the mean:
The inputs to the neural network would be, the mean, variance, skewness, kurtosis, time. The output is just the dm/dt.
But note that all the other derivativs i.e., dv/dt, ds/dt, dk/dt are gotten from the data. and dt/dt is set to be one.


parameters:

parser = argparse.ArgumentParser('ODE demo')
parser.add_argument('--method', type=str, choices=['dopri5', 'adams'], default='dopri5')
parser.add_argument('--data_size', type=int, default=1000)
parser.add_argument('--batch_time', type=int, default=15)
parser.add_argument('--batch_size', type=int, default=100)
parser.add_argument('--niters', type=int, default=10000)
parser.add_argument('--test_freq', type=int, default=20)
parser.add_argument('--viz', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
parser.add_argument('--adjoint', action='store_true')


Network archiecture:
def __init__(self):
        super(ODEFuncSkewness, self).__init__()

        self.net = nn.Sequential(
            nn.Linear(5, 64),
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, 1)
        )
        
        
##Learning rate for mean:
first 1220 = 0.005
from 1220-2000 =0.001

##Learning rate for Variance:
first 340 = 0.005
from 340-2000 =0.0005

##Learning rate for skewness:
first 280 = 0.005
from 280-2000 =0.0005

##Learning rate for Kurtosis:
first 1180 = 0.0005
from 1220-2000 =0.0001
